# Meta Overfitting Predictor

A machine learning project that trains neural networks on MNIST and builds a meta-learner to predict which models will overfit based on early training signals.

## Overview

**What is being predicted?** Whether a CNN model will overfit when fully trained, by analyzing only the **first 10 epochs** of training.

**How does it work?**

1. Train multiple CNN models on MNIST with different hyperparameters
2. Record training logs (loss, accuracy, generalization gap) during training
3. Manually label which models actually overfit after full training
4. Extract features from the **first 10 epochs only** (gap trends, accuracy slopes, etc.)
5. Train a Random Forest model to learn: _"given these early signals, will this model overfit?"_
6. Use the trained meta-model to **predict overfitting on new unseen training runs**

**The Key Insight:** You don't need to train a model to completion to know it will overfit—the pattern emerges in the first 10 epochs.

**Pipeline:**

```
1. Train CNN models → Get training logs
2. Label which ones overfit → Create metadata
3. Extract early signals → Calculate features
4. Train meta-model → Learn prediction rules
5. Demonstrate → Show predictions match reality
```

## Project Structure

```
meta-overfit-predictor/
├── README.md
├── requirements.txt
├── data/
│   └── MNIST/                      # MNIST dataset (raw)
│       └── raw/
├── training_Runs/
│   └── train_cnn_mnist.py          # CNN training script
├── logs/
│   ├── run_001.csv                 # Training logs (loss, accuracy, etc.)
│   ├── run_002.csv
│   └── ...                         # One CSV per training run
├── meta_dataset/
│   ├── extract_features.py         # Feature extraction from training logs
│   ├── meta_dataset.csv            # Run metadata and labels
│   └── meta_features.csv           # Extracted features for meta-learning
├── meta_models/
│   ├── random_forest.py            # Random Forest meta-model
│   └── baseline_logistic.py        # Logistic Regression baseline
├── labeling/
│   └── label_runs.py               # Script to label overfitting
└── notebooks/
    └── meta_dataset_eda.ipynb      # Exploratory Data Analysis
```

---

## Base Model and Data

- Dataset: MNIST
- Model: small CNN implemented in PyTorch
- Metrics logged per epoch:
  - training loss
  - validation loss
  - training accuracy
  - validation accuracy
  - generalization gap

Multiple training runs are generated by varying:
- number of epochs
- learning rate
- dataset size
- regularization settings

Each training run produces one CSV file inside the logs folder.

---

## Overfitting Labeling

Each training run is labeled using deterministic and explainable rules.

A run is labeled as overfitting if any of the following conditions hold:
- final generalization gap exceeds a threshold
- training accuracy approaches 100 percent while validation accuracy lags
- validation accuracy peaks and then degrades while training accuracy continues to improve

The result is stored in meta_dataset/meta_dataset.csv.

---

## Feature Engineering

Only early training behavior is used.

From the first 10 epochs, the following features are extracted:
- mean generalization gap
- maximum generalization gap
- slope of gap growth
- training accuracy slope
- validation accuracy slope
- training loss slope
- validation loss slope
- early training accuracy
- early validation accuracy

Each row in meta_features.csv represents one training run.

---

## Meta Models

### Logistic Regression
- Used as an interpretable baseline
- Helps identify which early signals correlate with overfitting
- Confirms that early generalization gap and memorization speed are meaningful signals

### Random Forest
- Non linear baseline model
- Captures interactions between early training signals
- Feature importance highlights gap based and memorization based features

---

## Evaluation Strategy

Because the number of training runs is small, leave one out cross validation is used.

For each fold:
- the model is trained on all but one run
- prediction is made on an unseen training run

This provides a strict and honest evaluation of generalization.

---

## Results and Findings

- Meta models perfectly explain the full dataset, confirming that early training signals are informative
- Under leave one out evaluation, both logistic regression and random forest struggle to detect overfitting consistently
- The primary bottleneck is the scarcity of overfitting examples, not feature quality or model choice
- Feature importance consistently highlights:
  - early generalization gap
  - gap growth rate
  - rapid memorization behavior

This demonstrates that overfitting prediction is a rare event detection problem.

---

## Limitations

- Very small number of overfitting examples
- Extreme class imbalance
- Single dataset
- Single architecture

These limitations are intentionally surfaced through strict evaluation rather than hidden.

---

## Future Work

- Generate more diverse overfitting runs
- Extend to CIFAR 10 and deeper architectures
- Reframe prediction as overfitting risk scoring
- Apply sequence models to full training curves
- Integrate with early stopping systems

---

## Why This Project Matters

Most ML projects optimize accuracy.  
This project focuses on predicting when training will fail.

It demonstrates:
- correct ML problem formulation
- experiment design
- feature engineering from training dynamics
- honest evaluation
- meta learning mindset

---

## Tech Stack

- Python
- PyTorch
- scikit learn
- NumPy
- Pandas
- Matplotlib
- Seaborn
- Jupyter

---

## How to Run

```bash
pip install -r requirements.txt

python training_runs/train_cnn_mnist.py
python labeling/label_runs.py
python meta_dataset/extract_features.py
python meta_models/baseline_logistic.py
python meta_models/random_forest.py